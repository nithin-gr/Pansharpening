{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pansformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yMBJf3FshNax",
        "izkIPHAdhJvb",
        "2nN0Nzzc2Enj",
        "kQJxGxaIxK8_",
        "bWnkIjc0xblg",
        "QIU4yo04xwQG",
        "u0qWwJg5x2UB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TtV7LVN2axE"
      },
      "source": [
        "## PANSFORMERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMBJf3FshNax"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VrwojhEXAQq"
      },
      "source": [
        "!pip install rasterio\n",
        "!pip install sewar\n",
        "import tifffile \n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import cv2\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.nn.functional import relu, leaky_relu, sigmoid\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, BatchNorm1d, Conv2d, ConvTranspose2d, MultiheadAttention, Softmax, Softmax2d, Container, Module, ModuleList\n",
        "from typing import Optional, Any\n",
        "import torch.cuda\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import Dropout, Linear, LayerNorm, LogSoftmax\n",
        "from torch.nn.functional import softmax , log_softmax\n",
        "import rasterio\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import torch.cuda\n",
        "from sewar.full_ref import mse, rmse, psnr, uqi, ssim, scc, sam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFduYX9CyeZ2"
      },
      "source": [
        "if torch.cuda.is_available(): # Setting up GPU Interface \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"gpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izkIPHAdhJvb"
      },
      "source": [
        "# Dataset Preprocessing (Array Creation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X0-NqIJfMby"
      },
      "source": [
        "def training_image_creation(img_ms, img_pan, n_factor):\n",
        "    \"\"\" \n",
        "    This function generates the blurred version of the original input multispectral image, and concatenate it with the \n",
        "    downsampled panchromatic so as to create the training sample used for Pansharpening Convolutional Neural Network (PCNN) \n",
        "    model training. \n",
        "    \n",
        "    Inputs:\n",
        "    - img_ms: Numpy array of the original multispectral image which is to be used for PCNN model training\n",
        "    - img_pan: Numpy array of the original panchromatic image which is to be used for PCNN model training\n",
        "    - n_factor: The ratio of pixel resolution of multispectral image to that of the panchromatic image\n",
        "    \n",
        "    Outputs:\n",
        "    - training_sample_array: Numpy array of concatenated blurred multispectral image and downsampled panchromatic image to be \n",
        "                             used for PCNN model training\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    blurred_img_ms = np.zeros((img_ms.shape))\n",
        "    \n",
        "    for i in range(img_ms.shape[2]):\n",
        "        blurred_img_ms[:, :, i] = cv2.GaussianBlur(img_ms[:, :, i], (3, 3), 0)\n",
        "    \n",
        "    blurred_img_ms_small = cv2.resize(blurred_img_ms, (int(img_ms.shape[1] / n_factor), int(img_ms.shape[0] / n_factor)), \n",
        "                                        interpolation = cv2.INTER_AREA)\n",
        "    blurred_img_ms_sam = cv2.resize(blurred_img_ms_small, (img_ms.shape[1], img_ms.shape[0]), interpolation = cv2.INTER_CUBIC)\n",
        "        \n",
        "    downsampled_img_pan = cv2.resize(img_pan, (img_ms.shape[1], img_ms.shape[0]), \n",
        "                                        interpolation = cv2.INTER_AREA)[:, :, np.newaxis]\n",
        "        \n",
        "    training_sample_array = np.concatenate((blurred_img_ms_sam, downsampled_img_pan), axis = 2)\n",
        "        \n",
        "    return training_sample_array\n",
        "\n",
        "\n",
        "\n",
        "def image_clip_to_segment(image_ms_array, train_image_array, image_height_size, image_width_size, percentage_overlap, \n",
        "                          buffer):\n",
        "    \"\"\" \n",
        "    This function is used to cut up original input images of any size into segments of a fixed size, with empty clipped areas \n",
        "    padded with zeros to ensure that segments are of equal fixed sizes and contain valid data values. The function then \n",
        "    returns a 4 - dimensional array containing the entire original input multispectral image and its corresponding \n",
        "    training image in the form of fixed size segments as training data inputs for the PCNN model.\n",
        "    \n",
        "    Inputs:\n",
        "    - image_ms_array: Numpy array of original input multispectral image to be used for PCNN model training\n",
        "    - train_image_array: Numpy array of training sample images to be used for PCNN model training\n",
        "    - image_height_size: Height of image to be fed into the PCNN model for training\n",
        "    - image_width_size: Width of image to be fed into the PCNN model for training\n",
        "    - percentage_overlap: Percentage of overlap between image patches extracted by sliding window to be used for model \n",
        "                          training\n",
        "    - buffer: Percentage allowance for image patch to be populated by reflected values for positions with no valid data values\n",
        "    \n",
        "    Output:\n",
        "    - train_segment_array: 4 - Dimensional numpy array of training sample images to serve as training data for PCNN model\n",
        "    - image_ms_segment_array: 4 - Dimensional numpy array of original input multispectral image to serve as target data for \n",
        "                           training PCNN model\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    y_size = ((image_ms_array.shape[0] // image_height_size) + 1) * image_height_size\n",
        "    y_pad = int(y_size - image_ms_array.shape[0])\n",
        "    x_size = ((image_ms_array.shape[1] // image_width_size) + 1) * image_width_size\n",
        "    x_pad = int(x_size - image_ms_array.shape[1])\n",
        "    \n",
        "    img_complete = np.pad(image_ms_array, ((0, y_pad), (0, x_pad), (0, 0)), mode = 'symmetric').astype(image_ms_array.dtype)\n",
        "    train_complete = np.pad(train_image_array, ((0, y_pad), (0, x_pad), (0, 0)), \n",
        "                            mode = 'symmetric').astype(train_image_array.dtype)\n",
        "        \n",
        "    img_list = []\n",
        "    train_list = []\n",
        "    \n",
        "    for i in range(0, int(img_complete.shape[0] - (2 - buffer) * image_height_size), \n",
        "                   int((1 - percentage_overlap) * image_height_size)):\n",
        "        for j in range(0, int(img_complete.shape[1] - (2 - buffer) * image_width_size), \n",
        "                       int((1 - percentage_overlap) * image_width_size)):\n",
        "            img_original = img_complete[i : i + image_height_size, j : j + image_width_size, 0 : image_ms_array.shape[2]]\n",
        "            img_list.append(img_original)\n",
        "            train_original = train_complete[i : i + image_height_size, j : j + image_width_size, :]\n",
        "            train_list.append(train_original)\n",
        "    \n",
        "    image_segment_array = np.zeros((len(img_list), image_height_size, image_width_size, image_ms_array.shape[2]))\n",
        "    train_segment_array = np.zeros((len(train_list), image_height_size, image_width_size, train_image_array.shape[2]))\n",
        "    \n",
        "    for index in range(len(img_list)):\n",
        "        image_segment_array[index] = img_list[index]\n",
        "        train_segment_array[index] = train_list[index]\n",
        "        \n",
        "    return train_segment_array, image_segment_array\n",
        "\n",
        "\n",
        "\n",
        "def training_data_generation(DATA_DIR, img_height_size, img_width_size, perc, buff, img_num):\n",
        "    \"\"\" \n",
        "    This function is used to read in files from a folder which contains the images which are to be used for training the \n",
        "    PCNN model, then returns 2 numpy arrays containing the training and target data for all the images in the folder so that\n",
        "    they can be used for PCNN model training.\n",
        "    \n",
        "    Inputs:\n",
        "    - DATA_DIR: File path of the folder containing the images to be used as training data for PCNN model.\n",
        "    - img_height_size: Height of image segment to be used for PCNN model training\n",
        "    - img_width_size: Width of image segment to be used for PCNN model training\n",
        "    - perc: Percentage of overlap between image patches extracted by sliding window to be used for model training\n",
        "    - buff: Percentage allowance for image patch to be populated by reflected values for positions with no valid data values\n",
        "    \n",
        "    Outputs:\n",
        "    - train_full_array: 4 - Dimensional numpy array of concatenated multispectral and downsampled panchromatic images to serve as \n",
        "                            training data for PCNN model\n",
        "    - img_full_array: 4 - Dimensional numpy array of original input multispectral image to serve as target data for training PCNN model\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if perc < 0 or perc > 1:\n",
        "        raise ValueError('Please input a number between 0 and 1 (inclusive) for perc.')\n",
        "        \n",
        "    if buff < 0 or buff > 1:\n",
        "        raise ValueError('Please input a number between 0 and 1 (inclusive) for buff.')\n",
        "\n",
        "    img_MS_files = glob.glob(DATA_DIR + 'MS/MS_' + str(img_num) +'.tif')\n",
        "    img_PAN_files = glob.glob(DATA_DIR + 'PAN/PAN_' + str(img_num) + '.tif')\n",
        "    \n",
        "    img_array_list = []\n",
        "    train_array_list = []\n",
        "    \n",
        "    for file in range(len(img_MS_files)):\n",
        "        \n",
        "        with rasterio.open(img_MS_files[file]) as f:\n",
        "            metadata = f.profile\n",
        "            ms_img = np.transpose(f.read(tuple(np.arange(metadata['count']) + 1)), [1, 2, 0])\n",
        "        with rasterio.open(img_PAN_files[file]) as g:\n",
        "            metadata_pan = g.profile\n",
        "            pan_img = g.read(1)\n",
        "            \n",
        "        ms_to_pan_ratio = metadata['transform'][0] / metadata_pan['transform'][0]\n",
        "            \n",
        "        train_img = training_image_creation(ms_img, pan_img, n_factor = ms_to_pan_ratio)\n",
        "    \n",
        "        train_array, img_array = image_clip_to_segment(ms_img, train_img, img_height_size, img_width_size, \n",
        "                                                       percentage_overlap = perc, buffer = buff)\n",
        "\n",
        "        img_array_list.append(img_array)\n",
        "        train_array_list.append(train_array)\n",
        "        del train_img\n",
        "        del train_array\n",
        "        del img_array\n",
        "\n",
        "    img_full_array = np.concatenate(img_array_list, axis = 0)\n",
        "    train_full_array = np.concatenate(train_array_list, axis = 0)\n",
        "    \n",
        "    del img_MS_files, img_PAN_files\n",
        "    gc.collect()\n",
        "    \n",
        "    return train_full_array, img_full_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNVaNNP711Zv"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p84UfgcI2gYX"
      },
      "source": [
        "Dataloader reads the image array generated previously to create batches of the image tiled. \n",
        "\n",
        "A common dataloader is written for loading training, validation and testing arrays. \n",
        "\n",
        "We have used memaps to improve the training speed. \n",
        "\n",
        "Memaps has to be created from the numpy array to use the dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKeppIKqgbR_"
      },
      "source": [
        "## IKONOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyGDfxi0fMUl"
      },
      "source": [
        "class IKONOS(Dataset):\n",
        "  def __init__(self, x_path, y_path, train=0, blur=0):\n",
        "    if train==0:\n",
        "        num_examples = 29854\n",
        "    elif train==1:\n",
        "        num_examples = 7434\n",
        "    elif train==2:\n",
        "        num_examples = 4066\n",
        "    self.x = np.memmap(x_path, dtype='float32', mode='r', shape=(num_examples, 5, 64, 64))\n",
        "    self.y = np.memmap(y_path, dtype='float32', mode='r', shape=(num_examples, 4, 64, 64))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return (self.x.shape[0])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input = self.x[idx]\n",
        "    gt = self.y[idx]\n",
        "\n",
        "    return (gt, input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFXKy62Zgg2T"
      },
      "source": [
        "ikonostrain = IKONOS(x_path = train_ip_path, y_path = train_gt_path, train=0, blur=0)\n",
        "ikonosval = IKONOS(x_path = val_ip_path, y_path = val_gt_path, train=1)\n",
        "ikonostest = IKONOS(x_path = test_ip_path, y_path = test_gt_path, train=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VN2dYVagmQo"
      },
      "source": [
        "train_batch_size = 32  #main control of batch size is here\n",
        "val_batch_size = 32\n",
        "test_batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXeCkg0kgnyP"
      },
      "source": [
        "ikonostrainparams = {\"batch_size\":train_batch_size, \n",
        "          \"shuffle\":True, \n",
        "          \"num_workers\":0}\n",
        "\n",
        "ikonosvalparams = {\"batch_size\":val_batch_size, \n",
        "          \"shuffle\":True, \n",
        "          \"num_workers\":0}\n",
        "          \n",
        "ikonostestparams = {\"batch_size\":test_batch_size, \n",
        "          \"shuffle\":True, \n",
        "          \"num_workers\":0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkUYrqBbgpoW"
      },
      "source": [
        "IkonosTrainDataloader = DataLoader(ikonostrain, **ikonostrainparams)\n",
        "IkonosValDataloader = DataLoader(ikonosval, **ikonosvalparams)\n",
        "IkonosTestDataloader = DataLoader(ikonostest, **ikonostestparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIdYS5OMvPnH"
      },
      "source": [
        "## LANDSAT-8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeQaaKNMvTT6"
      },
      "source": [
        "class Landsat(Dataset):\n",
        "  def __init__(self, x_path, y_path, train=0):\n",
        "    if train==0:\n",
        "        num_examples = 54268\n",
        "    elif train==1:\n",
        "        num_examples = 17955\n",
        "    elif train==2:\n",
        "        num_examples = 17955\n",
        "    self.x = np.memmap(x_path, dtype='float32', mode='r', shape=(num_examples, 5, 64, 64))\n",
        "    self.y = np.memmap(y_path, dtype='float32', mode='r', shape=(num_examples, 4, 64, 64))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return (self.x.shape[0])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input = self.x[idx]\n",
        "    gt = self.y[idx]\n",
        "\n",
        "    return (gt, input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nuWAh2OwehN"
      },
      "source": [
        "landsattrain = Landsat(x_path = train_ip_path, y_path = train_gt_path, train=0)\n",
        "landsatval = Landsat(x_path = val_ip_path, y_path = val_gt_path, train=1)\n",
        "landsattest = Landsat(x_path = test_ip_path, y_path = test_gt_path, train=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFruT_zvwhaZ"
      },
      "source": [
        "train_batch_size = 32  #main control of batch size is here\n",
        "val_batch_size = 64\n",
        "test_batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA8EoaAJwpfq"
      },
      "source": [
        "landsattrainparams = {\"batch_size\":train_batch_size, \n",
        "          \"shuffle\":True, \n",
        "          \"num_workers\":0}\n",
        "\n",
        "landsatvalparams = {\"batch_size\":val_batch_size, \n",
        "           \"shuffle\":True, \n",
        "           \"num_workers\":0}\n",
        "          \n",
        "landsattestparams = {\"batch_size\":test_batch_size, \n",
        "          \"shuffle\":True, \n",
        "          \"num_workers\":0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLOQlq9Dwtd9"
      },
      "source": [
        "LandsatTrainDataloader = DataLoader(landsattrain, **landsattrainparams)\n",
        "LandsatValDataloader = DataLoader(landsatval, **landsatvalparams)\n",
        "LandsatTestDataloader = DataLoader(landsattest, **landsattestparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nN0Nzzc2Enj"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "horEb8-Qwxcn"
      },
      "source": [
        "## Convolution PCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvRxtpo4w0pi"
      },
      "source": [
        "class ConvModelSeparate(nn.Module):\n",
        "    def __init__(self, final=False):\n",
        "\n",
        "        super(ConvModelSeparate, self).__init__()\n",
        "        #inputshape should be ideally (batchsize,5,256,256)\n",
        "        self.convlayer1 = nn.Conv2d(5, 64, (9,9), padding='same')\n",
        "        self.convlayer2 = nn.Conv2d(64, 32, (5,5), padding='same')\n",
        "        self.relu = nn.ReLU() #(input = 5x64x64, output = 16x128x128)\n",
        "            #self.batchnorm1 = nn.BatchNorm2d(5)\n",
        "\n",
        "        if final==True:\n",
        "            self.convlayer3 = nn.Conv2d(in_channels = 32, out_channels = 4, kernel_size = (5,5), padding='same') #(input = 32x64x64, output = 64x32x32) flatten = 16384\n",
        "            # self.batchnorm3 = nn.BatchNorm2d(5)\n",
        "\n",
        "        else:\n",
        "            self.convlayer3 = nn.Conv2d(32, 5, (5,5), padding='same')\n",
        "            #self.maxpool1 = nn.MaxPool2d(2, stride=None, padding = 0) #(input = 32x64x64, output = 64x32x32) flatten = 16384\n",
        "            #self.batchnorm1 = nn.BatchNorm2d(4)\n",
        "\n",
        "        \n",
        "        # self.convlayer4 = nn.Conv2d(in_channels = 64, out_channels = 100, kernel_size = 2, stride = 2) #(input = 64x32x32, output = 100x16x16)\n",
        "        # self.batchnorm4 = nn.BatchNorm2d(100)\n",
        "\n",
        "    def forward(self, input_image, final = False):\n",
        "        output = self.convlayer1(input_image)\n",
        "        output = self.relu(output)\n",
        "        output = self.convlayer2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.convlayer3(output)\n",
        "        if final==False:\n",
        "            output = self.relu(output)\n",
        "        #output = F.relu(self.batchnorm3(output))\n",
        "        #output = torch.flatten(output, start_dim = 2)\n",
        "        # print(output.shape)\n",
        "        # output = F.relu(self.batchnorm4(output))\n",
        "        # output = torch.reshape(output, (-1, 5, 5120))\n",
        "        #outputshape = (batchsize,5,8192) \n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJxGxaIxK8_"
      },
      "source": [
        "## Attention Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DxXIqyTxM4S"
      },
      "source": [
        "# Self Attention Module using Convolution Layers\n",
        "class ConvAttentionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ConvAttentionModule, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,padding='same')\n",
        "        self.k = torch.nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,padding='same')\n",
        "        self.v = torch.nn.Conv2d(in_channels,in_channels,kernel_size=1,stride=1,padding='same')\n",
        "   \n",
        "\n",
        "    def forward(self, x): \n",
        "\n",
        "        b,c,h,w = x.shape\n",
        "        q = self.q(x).reshape(b,c,h*w)\n",
        "        k = self.k(x).reshape(b,c,h*w)\n",
        "        v = self.v(x).reshape(b,c,h*w)\n",
        "\n",
        "        # compute attention\n",
        "        \n",
        "        q = q.permute(0,2,1)   # b,hw,c # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        # h_ = self.proj_out(h_)\n",
        "\n",
        "        return h_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q-8eUYWxRXg"
      },
      "source": [
        "# Only one of the classes given below must be run for a model \n",
        "\n",
        "class ConvMultiPatchAttention(nn.Module): #Multipatch attention class\n",
        "    def __init__(self, inchannels):\n",
        "        super(ConvMultiPatchAttention, self).__init__()\n",
        "        # self.simulatt = SimultaneousAttention(inchannels = inchannels)\n",
        "        self.attentionmodules = nn.ModuleList()\n",
        "        self.num_patches = 16\n",
        "        for _ in range(self.num_patches):\n",
        "            self.attentionmodules.append(ConvAttentionModule(in_channels=inchannels))\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self,image):\n",
        "        # a = torch.FloatTensor(image)\n",
        "        batch_size = image.shape[0]\n",
        "        patches = image.unfold(2, 16, 16).unfold(3,16,16)\n",
        "        unfold_shape = patches.size()\n",
        "        # print(unfold_shape)\n",
        "        patches = patches.contiguous().view(batch_size,-1,16,16,16)\n",
        "        patches = patches.permute(2,0,1,3,4)\n",
        "\n",
        "        attention_patches = torch.zeros(patches.size()).to(device)\n",
        "        for i in range(self.num_patches):\n",
        "            attention_patches[i] = self.attentionmodules[i](patches[i]) #patch wise attention function\n",
        "\n",
        "        patches = attention_patches.permute(2,0,1,3,4)\n",
        "        output_h = unfold_shape[2]*unfold_shape[4]\n",
        "        output_w = unfold_shape[3]*unfold_shape[5]\n",
        "\n",
        "        patches = patches.contiguous().view(batch_size, -1, 4, 4, 16, 16)\n",
        "        patches = patches.permute(0,1,2,4,3,5).contiguous()\n",
        "        orig_image = patches.view(batch_size, patches.shape[1], output_h, output_w)\n",
        "\n",
        "        return orig_image  #batchsize, 5, 64, 64\n",
        "\n",
        " class GlobalAttention(nn.Module): # Global Attention Class\n",
        "    def __init__(self, inchannels):\n",
        "         super(GlobalAttention, self).__init__()\n",
        "         self.globalatt = ConvAttentionModule(in_channels=inchannels)\n",
        "    \n",
        "     def forward(self, image):\n",
        "         output = self.global2,0,1att(image)\n",
        "\n",
        "         return output\n",
        "\n",
        "\n",
        " class CombinedAttentionLayer(nn.Module): # Global + Multipatch Attention Class\n",
        "     def __init__(self):\n",
        "         super(CombinedAttentionLayer, self).__init__()\n",
        "         self.mpa = ConvMultiPatchAttention()\n",
        "         self.ga = GlobalAttention()\n",
        "         self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                         in_channels,\n",
        "                                         kernel_size=1,\n",
        "                                         stride=1,\n",
        "                                         padding='same')\n",
        "\n",
        "    def forward(self, image)2,0,1:\n",
        "         mpa_output = self.mpa(image)\n",
        "         ga_output = self.ga(image)\n",
        "         combined_output = mpa_output + ga_output\n",
        "         final_prof = self.proj_out(combined_output)\n",
        "        \n",
        "         return final_proj "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWnkIjc0xblg"
      },
      "source": [
        "## Pansformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agj-E1S7xa3n"
      },
      "source": [
        "# Number of Layers of the models can be adjusted here\n",
        "# Corresponding Attention Layers must be used while running the model (ie; the current configuration is for MultiPatch Attention)\n",
        "\n",
        "class Pansformers(nn.Module):\n",
        "    def __init__(self, channels = 5):\n",
        "        super(Pansformers, self).__init__()\n",
        "        self.convlayer1 = ConvModelSeparate()\n",
        "        # self.convlayer2 = ConvModelSeparate()\n",
        "        # self.globalatt = ConvAttentionModule(in_channels = channels).to(device)\n",
        "        self.multipatchattention = ConvMultiPatchAttention(inchannels = channels).to(device)\n",
        "        # self.multipatchattention2 = ConvMultiPatchAttention(inchannels = channels).to(device)\n",
        "        self.projection1 = nn.Conv2d(channels, 20, kernel_size=1,stride=1,padding='same')\n",
        "        self.projection2 = nn.Conv2d(20, channels, kernel_size=1, stride=1, padding='same')\n",
        "        # self.convlayer3 = ConvModelSeparate()\n",
        "        self.finalconv = ConvModelSeparate(final=True)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        # src input shape (batch_size, 5, 256, 256)\n",
        "        output1 = self.convlayer1(src)\n",
        "        # output2 = self.convlayer2(output1)\n",
        "    \n",
        "\n",
        "        # ga_output = self.globalatt(output1)\n",
        "        mpa_output = self.multipatchattention(output1)\n",
        "        \n",
        "\n",
        "        # att_out_1 = att_proj_1 + output1\n",
        "\n",
        "        # mpa_output2 = self.multipatchattention2(att_out_1)\n",
        "\n",
        "        # combined_out = mpa_output + ga_output\n",
        "\n",
        "        att_proj_1 = self.projection1(mpa_output)\n",
        "        \n",
        "        att_proj_2 = self.projection2(att_proj_1)\n",
        "\n",
        "        output_att = att_proj_2 + output1  #skip connection\n",
        "\n",
        "\n",
        "        # output3 = self.convlayer3(output_att)\n",
        "        finaloutput = self.finalconv(output_att)\n",
        "\n",
        "        return finaloutput "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY2T6ju6xndd"
      },
      "source": [
        "# Loss function and Optimizer\n",
        "\n",
        "model = Pansformers(channels = 5).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,  betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0001, amsgrad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIU4yo04xwQG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY4obN6AxxrA"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "L2 = []\n",
        "num_epochs = 100\n",
        "for epoch in tqdm(range(61,num_epochs+1)):\n",
        "    model.train()\n",
        "    cumulative_loss = 0.0    \n",
        "    btch = 0\n",
        "    for i, generator_values in enumerate(LandsatTrainDataloader):  #model input (32,5,64,64)\n",
        "        groundtruth = generator_values[0].float().to(device)\n",
        "        inputs = generator_values[1].float().to(device)\n",
        "        model_output = model(inputs)\n",
        "        main_loss = criterion(model_output, groundtruth)\n",
        "        main_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        cumulative_loss += main_loss.item()*inputs.size(0)\n",
        "        btch += 1\n",
        "\n",
        "    epoch_loss = cumulative_loss/float(54268.0)\n",
        "    L2.append(epoch_loss)\n",
        "    print('End of Epoch Training Loss: ' , epoch_loss)\n",
        "    print('-----------------------------------------------------')\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "    with open('/content/drive/MyDrive/LANDSAT8/Outputs/Global_WithProjection_Blur3.txt', 'at') as file :       \n",
        "        now = datetime.datetime.now()\n",
        "        current_time = now.strftime(\"%H:%M:%S\")\n",
        "        file.write(\"Epoch: {}, Batch_Loss: {}, Loss: {}, Time: {}\\n\".format(epoch, main_loss.item(), cumulative_loss, current_time))     \n",
        "\n",
        "    # Validation Set\n",
        "\n",
        "    if epoch%5==0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            for i, generator_values in enumerate(LandsatValDataloader):  #model input (32,5,64,64)\n",
        "                val_gt = generator_values[0].float().to(device)\n",
        "                val_ip = generator_values[1].float().to(device)\n",
        "                predictions=model(val_ip)\n",
        "                loss = criterion(predictions, val_gt)\n",
        "                val_loss += loss.item()*val_ip.shape[0]\n",
        "            val_loss_final = val_loss/float(17955)\n",
        "            print(\"Validation loss: \", val_loss_final)\n",
        "            print(\"---------------------------------------------------\")\n",
        "    model.train()\n",
        "\n",
        "    del groundtruth\n",
        "    del inputs\n",
        "    del model_output\n",
        "\n",
        "   \n",
        "    if epoch%10==0:\n",
        "        torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'train_loss': epoch_loss,\n",
        "          'val_loss': val_loss\n",
        "          }, '/content/drive/MyDrive/LANDSAT8/Models/Global_WithProjection_Blur3.pt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qWwJg5x2UB"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abwdo7D6yAbH"
      },
      "source": [
        "# Function to determine metrics of the predicted images \n",
        "\n",
        "def FullRef_Metrics(predicted, groundtruth):\n",
        "    if groundtruth.shape == predicted.shape:\n",
        "        total = predicted.shape[0] \n",
        "    else:\n",
        "        print(\"Error: Array Shape Mismatch\")\n",
        "    names = ['MSE', 'RMSE', 'PSNR', 'UQI', 'SCC', 'SAM', 'SSIM']\n",
        "    metrics = [0] * len(names)\n",
        "    results = []\n",
        "    t = 0\n",
        "    for i in tqdm(range(0,total)):\n",
        "        predictedimage = predicted[i]\n",
        "        gtimage = groundtruth[i]\n",
        "        s = sam(gtimage, predictedimage)\n",
        "        if math.isnan(s) == True:\n",
        "            continue\n",
        "        else:\n",
        "            metrics[0] += mse(gtimage, predictedimage)\n",
        "            metrics[1] += rmse(gtimage, predictedimage)\n",
        "            metrics[2] += psnr(gtimage, predictedimage, MAX = 2047) # The value of MAX changes according to the bit level of the image\n",
        "            metrics[3] += uqi(gtimage, predictedimage, ws = 2)\n",
        "            metrics[4] += scc(gtimage, predictedimage)\n",
        "            metrics[5] += s\n",
        "            ssm, cs = ssim(gtimage, predictedimage, ws = 2, MAX = 2047)\n",
        "            metrics[6] += ssm\n",
        "            t += 1\n",
        "    for i in metrics:\n",
        "      results.append(i/t)\n",
        "    mets = {}\n",
        "    for i in range(0, len(names)):\n",
        "        mets[names[i]] = results[i]\n",
        "    l = list(mets.items())\n",
        "    dt = pd.DataFrame(l,columns = ['Metrics','Values'])\n",
        "    return dt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}